{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Word embedding.ipynb","provenance":[{"file_id":"1G0fenEUKbVszvAcXhT2XlwBEeUmnoVxJ","timestamp":1591805648539}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"eqw_NvWnNHq6","colab_type":"text"},"source":["# Word embedding manipulation"]},{"cell_type":"markdown","metadata":{"id":"svsBTr3zNPu4","colab_type":"text"},"source":["## Load pre-trained embeddings\n","\n","You could train your own word embedding (using library like [gensim](https://radimrehurek.com/gensim/models/word2vec.html))  if you want, however you would need a lot of text and you would have to determine a ton of parameters (What is the size of your context, how big do you want your embedding, which algorithm to use, etc.).\n","\n","Why go through all that hassle when you can just use embeddings that specialist in the field already trained on huge corpus?\n","\n","[SpaCy](https://spacy.io/usage/models) is a library for NLP that provide such embeddings.\n","\n","### Run the code bellow :"]},{"cell_type":"code","metadata":{"id":"l3YRZwRkM98k","colab_type":"code","outputId":"5134dd03-a523-4900-bd11-53da5b0c70c9","executionInfo":{"status":"ok","timestamp":1592203899496,"user_tz":-120,"elapsed":21965,"user":{"displayName":"Sébastien Vanstavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5NfGIDvXc-vdIHXhbJ3ezTYoxOfaUGnMRSECx5w=s64","userId":"10084807894626874113"}},"colab":{"base_uri":"https://localhost:8080/","height":481}},"source":["# Download the embeddings\n","\n","!python3 -m spacy download en_core_web_md\n","\n","# Load them\n","import nltk\n","nltk.download('stopwords')\n","\n","import en_core_web_md\n","nlp = en_core_web_md.load()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: en_core_web_md==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz#egg=en_core_web_md==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n","Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.6.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.2)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.18.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (47.1.1)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2020.4.5.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (1.6.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.1.0)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_md')\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"m2ie5NjIZ_uW","colab_type":"text"},"source":["### Some optionnal information on this model \n","\n","The word embeddings of this model are of size 300 (a pretty standard size) and are trained using [GloVe](https://mlexplained.com/2018/04/29/paper-dissected-glove-global-vectors-for-word-representation-explained/) algorithm. The model you loaded also come with other types of embeddings that may be useful for other NLP tasks (like Part Of speech vectors). \n","\n","There also exist a larger model with more words and models for other languages (see the SpaCy link)."]},{"cell_type":"markdown","metadata":{"id":"s7jXkmpINUkl","colab_type":"text"},"source":["## Tokens embeddings and similarity"]},{"cell_type":"markdown","metadata":{"id":"qIxKNZZHflq4","colab_type":"text"},"source":["Now that the model is loaded, we can give it a sentence and it will tokenise it and return a list of tokens with a number of attributes.\n","\n","Run the two following cells and try to understand them : "]},{"cell_type":"code","metadata":{"id":"l-FvGXNKN9Iq","colab_type":"code","outputId":"ca923149-5a4f-46c4-87eb-888bf91df359","executionInfo":{"status":"ok","timestamp":1592203899497,"user_tz":-120,"elapsed":21956,"user":{"displayName":"Sébastien Vanstavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5NfGIDvXc-vdIHXhbJ3ezTYoxOfaUGnMRSECx5w=s64","userId":"10084807894626874113"}},"colab":{"base_uri":"https://localhost:8080/","height":176}},"source":["tokens = nlp(\"Hello, I'm a data analyst. aabbbb\")\n","\n","for t in tokens:\n","    print(t.text, t.has_vector, t.vector_norm)\n","\n","# The attribute has_vector for \"aabbbb\" is False, it mean that no vector exist for this word in the model."],"execution_count":2,"outputs":[{"output_type":"stream","text":["Hello True 5.586428\n",", True 5.094723\n","I True 6.4231944\n","'m True 5.9417286\n","a True 5.306696\n","data True 7.1505103\n","analyst True 7.489983\n",". True 4.9316354\n","aabbbb False 0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_hDZxEBlhKoa","colab_type":"code","outputId":"5868af91-ad37-45b7-84b4-943cf1ed8534","executionInfo":{"status":"ok","timestamp":1592203899497,"user_tz":-120,"elapsed":21948,"user":{"displayName":"Sébastien Vanstavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5NfGIDvXc-vdIHXhbJ3ezTYoxOfaUGnMRSECx5w=s64","userId":"10084807894626874113"}},"colab":{"base_uri":"https://localhost:8080/","height":923}},"source":["print('Vector of \"' + tokens[0].text + '\" : \\n', tokens[0].vector)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Vector of \"Hello\" : \n"," [ 0.25233    0.10176   -0.67485    0.21117    0.43492    0.16542\n","  0.48261   -0.81222    0.041321   0.78502   -0.077857  -0.66324\n","  0.1464    -0.29289   -0.25488    0.019293  -0.20265    0.98232\n","  0.028312  -0.081276  -0.1214     0.13126   -0.17648    0.13556\n"," -0.16361   -0.22574    0.055006  -0.20308    0.20718    0.095785\n","  0.22481    0.21537   -0.32982   -0.12241   -0.40031   -0.079381\n"," -0.19958   -0.015083  -0.079139  -0.18132    0.20681   -0.36196\n"," -0.30744   -0.24422   -0.23113    0.09798    0.1463    -0.062738\n","  0.42934   -0.078038  -0.19627    0.65093   -0.22807   -0.30308\n"," -0.12483   -0.17568   -0.14651    0.15361   -0.29518    0.15099\n"," -0.51726   -0.033564  -0.23109   -0.7833     0.018029  -0.15719\n","  0.02293    0.49639    0.029225   0.05669    0.14616   -0.19195\n","  0.16244    0.23898    0.36431    0.45263    0.2456     0.23803\n","  0.31399    0.3487    -0.035791   0.56108   -0.25345    0.051964\n"," -0.10618   -0.30962    1.0585    -0.42025    0.18216   -0.11256\n","  0.40576    0.11784   -0.19705   -0.075292   0.080723  -0.02782\n"," -0.15617   -0.44681   -0.15165    0.1692     0.098255  -0.031894\n","  0.087143   0.26082    0.002706   0.1319     0.34439   -0.37894\n"," -0.4114     0.081571  -0.11674   -0.43711    0.011144   0.099353\n","  0.26612    0.40025    0.18895   -0.18438   -0.30355   -0.2725\n","  0.22468   -0.40614    0.15618   -0.16043    0.47147    0.0080203\n","  0.56858    0.21934   -0.11181    0.79925    0.10714   -0.50146\n","  0.063593   0.069465   0.15292   -0.2747    -0.20989    0.20737\n"," -0.10681    0.40651   -2.6438    -0.31139   -0.32157   -0.26458\n"," -0.35625    0.070013  -0.18838    0.48773   -0.26167   -0.020805\n","  0.17819    0.15758   -0.13752    0.056464   0.30766   -0.066136\n","  0.4748    -0.27335    0.09732   -0.20832    0.0039332  0.346\n"," -0.08702   -0.54924   -0.18759   -0.17174    0.060324  -0.13521\n","  0.10419    0.30165    0.05798    0.21872   -0.073594  -0.20423\n"," -0.25279   -0.10471   -0.32163    0.12525   -0.31281    0.0097207\n"," -0.26777   -0.61121   -0.11089   -0.13652    0.035135  -0.4939\n","  0.084857  -0.15494   -0.063509  -0.23935    0.28272    0.10849\n"," -0.3365    -0.60764    0.38576   -0.0095438  0.17499   -0.52723\n","  0.62211    0.19544   -0.48977    0.036582  -0.128     -0.016827\n","  0.25647   -0.31698    0.48257   -0.14184    0.11046   -0.3098\n"," -0.63141   -0.37268    0.23183   -0.14268   -0.02341    0.022255\n"," -0.044662  -0.16404   -0.25848    0.1629     0.024751   0.23348\n","  0.27933    0.38998   -0.058968   0.11355    0.15673    0.18583\n"," -0.19814   -0.48123   -0.035084   0.078458  -0.49833    0.10855\n"," -0.20133    0.05292   -0.11583   -0.16009    0.16768    0.42362\n"," -0.23106    0.082465   0.24296   -0.16786    0.0080409  0.085947\n","  0.38033    0.072981   0.1633     0.24704   -0.11094    0.15115\n"," -0.22068   -0.061944  -0.037091  -0.087923  -0.23181    0.15035\n"," -0.19093   -0.19113   -0.11894    0.094908  -0.0043347  0.15362\n"," -0.41201   -0.3073     0.18375    0.40206   -0.0034793 -0.10917\n"," -0.69522    0.10161   -0.079256   0.40329    0.22285   -0.19374\n"," -0.13315    0.073231   0.099832   0.11685   -0.21643   -0.1108\n","  0.10341    0.097286   0.11196   -0.3894    -0.0089363  0.28809\n"," -0.10792    0.028811   0.32545    0.26052   -0.038941   0.075204\n","  0.46031   -0.06293    0.21661    0.17869   -0.51917    0.33591  ]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8HYFTWYfgsIr","colab_type":"text"},"source":["You can also get the similarity between two tokens."]},{"cell_type":"code","metadata":{"id":"CVrXPY9xgoWw","colab_type":"code","outputId":"4ada6422-6e2f-40da-8fd8-dfc53884e52c","executionInfo":{"status":"ok","timestamp":1592203899498,"user_tz":-120,"elapsed":21939,"user":{"displayName":"Sébastien Vanstavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5NfGIDvXc-vdIHXhbJ3ezTYoxOfaUGnMRSECx5w=s64","userId":"10084807894626874113"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["tokens = nlp(\"dog cat banana\")\n","\n","for i in range(len(tokens)):\n","    for j in range(i+1, len(tokens)):\n","        print(tokens[i].text, tokens[j].text, tokens[i].similarity(tokens[j]))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["dog cat 0.80168545\n","dog banana 0.24327643\n","cat banana 0.28154364\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Bk4vnFT3kdJ8","colab_type":"text"},"source":["**Warning** : You may find other pre-trained embeddings that you want to use or even train your owns with another library. All library has different methods, attributes and ways of handling embeddings, read the documentation and examples before using them."]},{"cell_type":"markdown","metadata":{"id":"NWxsDQZDN93H","colab_type":"text"},"source":["# Sentence embeddings"]},{"cell_type":"markdown","metadata":{"id":"6-J_6NjCmyzH","colab_type":"text"},"source":["Now you know how to manipulate word embeddings, congratulation. \n","So you have the sentence that you want to classify, and you have the embedding of each word of this sentence... Now what?\n","\n","Maybe you can concatenate all of these vectors and just give it to the classifier? \n","\n","Problems: \n","\n","- It would give a very very big vector. \n","\n","- It would be EXTREMELY sensible of the orders of the words \n","\n","- You would have to handle sentence having difference size with padding.\n","\n","In practice, state of the art model either train special sentence embeddings for their task or use special sequential neural network (RNN/LSTM). \n","\n","But we won't do that here (phew!). Actually just doing the average of the vectors works surprisingly well. And good news spacy comes with this functionality!"]},{"cell_type":"code","metadata":{"id":"OHtFgFAkOBpn","colab_type":"code","outputId":"061e9598-e31c-4868-f161-b49d49405c30","executionInfo":{"status":"ok","timestamp":1592203899498,"user_tz":-120,"elapsed":21934,"user":{"displayName":"Sébastien Vanstavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5NfGIDvXc-vdIHXhbJ3ezTYoxOfaUGnMRSECx5w=s64","userId":"10084807894626874113"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["tokens = nlp(\"Hello, I am a sentence.\")\n","tokens.vector"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 1.44052897e-02,  3.73151302e-01, -3.24717134e-01, -1.14375697e-02,\n","        1.16910569e-01,  1.52780011e-01,  1.90909535e-01, -4.37690973e-01,\n","       -3.23535725e-02,  1.87437439e+00, -2.64555395e-01,  1.18573561e-01,\n","        3.17365713e-02, -8.37386176e-02, -1.97842836e-01, -5.29319979e-02,\n","       -1.00312009e-01,  1.20892560e+00, -8.57821181e-02,  5.27822860e-02,\n","       -7.06457123e-02, -1.33178145e-01,  4.75231409e-02,  1.12724580e-01,\n","       -1.40458560e-02, -2.75365766e-02, -6.30009994e-02, -1.70811281e-01,\n","        4.11111683e-01, -6.48657158e-02, -1.10077433e-01,  1.48285711e-02,\n","       -7.37885758e-02,  8.64421576e-02, -5.08786440e-02, -8.35811496e-02,\n","        9.39526111e-02, -5.84518574e-02, -2.87427843e-01, -1.23048410e-01,\n","        5.54607138e-02, -2.00897142e-01,  6.62029907e-02, -8.45435113e-02,\n","        5.04015684e-02,  8.26912746e-02, -1.58224300e-01, -1.18329972e-02,\n","        1.42642856e-01, -1.73901469e-02, -1.13422573e-01,  3.16255152e-01,\n","       -3.69520001e-02,  7.68807083e-02,  1.71735715e-02,  1.60220146e-01,\n","       -9.47662815e-02, -9.83557105e-02,  2.65226476e-02, -5.55135719e-02,\n","       -2.07957506e-01, -1.27011985e-01,  2.07212865e-02, -1.24535691e-02,\n","       -1.43218994e-01, -2.82448977e-01,  1.86614133e-02,  1.43119141e-01,\n","        3.06205191e-02,  2.65296280e-01,  1.76842865e-02, -8.55228528e-02,\n","        5.93550019e-02, -6.59179911e-02,  1.19104154e-01,  8.99427384e-02,\n","        1.27255857e-01,  1.01765739e-02, -1.79712862e-01,  3.39434296e-01,\n","       -4.62475717e-02,  1.28500730e-01, -4.87297103e-02,  2.92480495e-02,\n","        6.14728518e-02, -1.79125771e-01,  5.53734303e-01, -5.01040161e-01,\n","        3.65188569e-01,  7.36894235e-02,  9.31634381e-02, -3.76090035e-02,\n","       -1.65330753e-01, -1.55868586e-02,  1.29011855e-01,  5.35494201e-02,\n","        2.71814298e-02, -5.44671416e-02, -6.10171370e-02, -5.45624234e-02,\n","       -2.55282838e-02,  8.23971443e-03, -1.20035283e-01, -2.36151405e-02,\n","        1.09951720e-01, -2.40114287e-01,  1.18401431e-01, -3.06479841e-01,\n","       -2.80191433e-02,  7.56515637e-02, -1.06442861e-01, -1.38922855e-01,\n","        7.64508471e-02, -1.82348147e-01, -2.49890089e-02,  2.60883123e-02,\n","       -8.30445737e-02, -1.66031837e-01, -9.80639979e-02, -1.61273852e-01,\n","        1.76673725e-01, -1.01017289e-01,  1.23424277e-01, -4.33896594e-02,\n","        1.19256273e-01,  2.63628185e-01, -5.55705763e-02, -1.44293293e-01,\n","       -5.43135069e-02,  2.34199688e-01,  1.28148571e-01, -1.63928583e-01,\n","       -1.47400290e-01,  5.17424271e-02,  1.64506301e-01, -6.10374920e-02,\n","       -1.94647148e-01,  5.53388558e-02, -8.37032795e-02,  7.23114163e-02,\n","       -1.52108991e+00,  1.69426128e-01,  9.52285752e-02,  3.31528438e-03,\n","       -1.29208580e-01,  1.67757168e-01, -2.03659147e-01,  5.62857091e-03,\n","       -3.47591452e-02, -2.29628429e-01, -6.69087172e-02,  2.02227123e-02,\n","        1.03534281e-01,  1.81040287e-01, -1.01627887e-03,  5.35194278e-02,\n","        6.57549277e-02, -2.36177012e-01,  1.27344280e-01, -3.38239968e-02,\n","        4.54488918e-02, -2.38978714e-02,  6.00197129e-02, -1.33427575e-01,\n","       -2.25813985e-01, -4.88330014e-02, -4.76578586e-02, -2.35921424e-02,\n","        2.21651401e-02,  8.90268534e-02, -5.43328561e-02, -1.41534284e-01,\n","        4.88594286e-02, -1.77055284e-01, -9.59116295e-02, -5.22658564e-02,\n","       -1.43747702e-01, -3.52611393e-02, -1.37364313e-01, -3.53048146e-02,\n","       -1.11246422e-01, -1.92720562e-01, -2.18152568e-01, -8.46391395e-02,\n","        1.74741428e-02, -1.70696657e-02, -3.63935754e-02, -1.56151457e-02,\n","        8.68555754e-02, -7.98735693e-02, -1.66885667e-02,  1.47202715e-01,\n","       -1.54805988e-01, -1.99369997e-01,  1.67219996e-01,  1.71863899e-01,\n","       -1.06638521e-02, -2.72651434e-01,  2.15161443e-02, -7.87457153e-02,\n","       -1.44534260e-02, -7.17248544e-02, -1.55521736e-01,  6.93697184e-02,\n","        2.34216407e-01, -1.05746992e-01,  1.40627578e-01, -7.09802434e-02,\n","        5.79578616e-02,  3.08185723e-02, -1.45537421e-01, -8.20735693e-02,\n","        2.76152883e-02, -1.47826150e-01,  3.12319130e-01,  1.30597308e-01,\n","       -1.56094864e-01,  3.45361941e-02, -1.66462854e-01,  2.20469996e-01,\n","        9.80031416e-02, -1.53056914e-02,  8.69217068e-02,  2.04923004e-01,\n","        1.32285699e-03, -6.07662797e-02,  9.15271416e-02,  1.05157152e-01,\n","       -6.43650070e-02, -2.28678584e-01, -1.07657291e-01,  1.17138147e-01,\n","       -1.42655566e-01,  1.34951577e-01, -3.62164259e-01, -2.02744856e-01,\n","       -2.30857566e-01,  6.58865795e-02, -7.93944225e-02,  2.59301454e-01,\n","        7.07602873e-02,  6.96412846e-02,  1.12779431e-01,  1.78258549e-02,\n","        7.66722634e-02, -7.34422877e-02,  2.07327440e-01, -1.11784145e-01,\n","       -5.62185794e-03,  3.14647287e-01,  1.49668586e-02, -2.10235119e-02,\n","       -6.12544231e-02,  8.31890032e-02,  1.26132295e-01,  1.07743286e-01,\n","        4.57694717e-02, -9.54041332e-02, -4.39302996e-02, -1.17429994e-01,\n","        6.84772804e-02,  1.69816718e-01,  1.81013897e-01,  1.49651572e-01,\n","       -1.60226852e-01, -1.83162704e-01, -2.50668563e-02,  1.38822660e-01,\n","        2.69179672e-01, -8.62550065e-02, -2.05171704e-01, -1.52211428e-01,\n","       -1.37221992e-01,  9.57540125e-02, -1.04001999e-01, -5.54075763e-02,\n","       -4.19071503e-02, -1.06302857e-01,  3.75687666e-02,  2.98337549e-01,\n","        3.57440025e-01, -1.30178720e-01,  9.90604311e-02, -2.34011427e-01,\n","       -9.40744281e-02, -1.54106572e-01,  4.22973819e-02, -3.70557196e-02,\n","        1.97939992e-01,  8.75375867e-02,  8.03851485e-02,  6.07320555e-02,\n","       -1.92981571e-01, -5.63905723e-02,  2.22948529e-02, -5.04177138e-02,\n","       -9.93956812e-03, -1.61464233e-02, -2.40555719e-01,  1.76847130e-01],\n","      dtype=float32)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"IG1IN73AtJKs","colab_type":"text"},"source":["You can also get sentences similarity."]},{"cell_type":"code","metadata":{"id":"VCD70Bf4tSYA","colab_type":"code","outputId":"4db0593f-cbe5-41bf-81eb-91cc597fd954","executionInfo":{"status":"ok","timestamp":1592203899499,"user_tz":-120,"elapsed":21930,"user":{"displayName":"Sébastien Vanstavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5NfGIDvXc-vdIHXhbJ3ezTYoxOfaUGnMRSECx5w=s64","userId":"10084807894626874113"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["tokens1 = nlp(\"Hello, I am a sentence.\")\n","tokens2 = nlp(\"Hi, also some sort of phrase!\")\n","tokens3 = nlp(\"This cat is cute.\")\n","\n","print(tokens1.similarity(tokens2))\n","print(tokens1.similarity(tokens3))\n","print(tokens2.similarity(tokens3))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["0.832282210939598\n","0.7502564755692778\n","0.7618915522647609\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-KU8ZHKYvIYz","colab_type":"text"},"source":["Just doing a mere average on untreated sentence actually have one problem: it gives to much weight to stop word or other very frequent and not important words. \n","\n","That is why you should delete the stop words like you did previously.\n","\n","Try to do it now and compute the embeddings for each treated sentences : "]},{"cell_type":"code","metadata":{"id":"WRP3vH7SpkdB","colab_type":"code","colab":{}},"source":["def PunkStopWordsExterm(sentence, output='sentence'):\n","  '''\n","  Exterminate punctuation and stop words of a sentence and return as a sentence\n","  or list of words. \n","\n","  :output: str : 'sentence' or 'words'\n","  'sentence'  would return the result as a sentence\n","  'words'     would return the result as a list of words\n","  '''\n","  # import nltk\n","  # nltk.download('stopwords')\n","\n","  from nltk.corpus import stopwords\n","  from nltk.tokenize import RegexpTokenizer\n","\n","  tokenizer = RegexpTokenizer(r'\\w+')\n","  words = tokenizer.tokenize(sentence)\n","\n","  stop_words = set(stopwords.words(\"english\"))\n","\n","  if output == 'sentence':\n","    return \" \".join([w for w in words if not w in stop_words])\n","  elif output == 'words':\n","    return [w for w in words if not w in stop_words]\n","  else:\n","    Warning(\n","    print(\"Error : Wrong output format selected.\", \n","                   \"Please chose between 'sentence' or 'words'!\"))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gnRzdEPj0_Ig","colab_type":"code","outputId":"f828e9c3-1fee-4ce8-ad09-9e74a8226433","executionInfo":{"status":"ok","timestamp":1592203899500,"user_tz":-120,"elapsed":21922,"user":{"displayName":"Sébastien Vanstavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5NfGIDvXc-vdIHXhbJ3ezTYoxOfaUGnMRSECx5w=s64","userId":"10084807894626874113"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["sentence = \"Hello, I am a sentence.\"\n","\n","PunkStopWordsExterm(sentence, 'sentence')"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Hello I sentence'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"HJS7UcVR0_uD","colab_type":"code","outputId":"4311954f-c45b-4957-ae72-7130b91af76e","executionInfo":{"status":"ok","timestamp":1592203899500,"user_tz":-120,"elapsed":21917,"user":{"displayName":"Sébastien Vanstavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5NfGIDvXc-vdIHXhbJ3ezTYoxOfaUGnMRSECx5w=s64","userId":"10084807894626874113"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["PunkStopWordsExterm(sentence, 'words')"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hello', 'I', 'sentence']"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"T5qottQW1qnk","colab_type":"code","outputId":"8382be29-34a5-474a-e42d-bb7e8c8dbaa6","executionInfo":{"status":"ok","timestamp":1592203899501,"user_tz":-120,"elapsed":21913,"user":{"displayName":"Sébastien Vanstavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5NfGIDvXc-vdIHXhbJ3ezTYoxOfaUGnMRSECx5w=s64","userId":"10084807894626874113"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["PunkStopWordsExterm(sentence, 'word')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Error : Wrong output format selected. Please chose between 'sentence' or 'words'!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BST4sFi7vKbC","colab_type":"code","outputId":"f1a62fb8-a551-4238-b313-34d546894579","executionInfo":{"status":"ok","timestamp":1592203899501,"user_tz":-120,"elapsed":21906,"user":{"displayName":"Sébastien Vanstavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5NfGIDvXc-vdIHXhbJ3ezTYoxOfaUGnMRSECx5w=s64","userId":"10084807894626874113"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["# On mets les phrases dans un corpus pour les traiter plus vite\n","corpus = [\"Hello, I am a sentence.\", \"Hi, also some sort of phrase!\", \n","          \"This cat is cute.\"]\n","\n","# On créé un nouveau corpus en appliquant les fonction de filtre\n","fltd_tok_corp = [nlp(PunkStopWordsExterm(sent, 'sentence')) for sent in corpus]\n","\n","# On compare les token du corpus filtré\n","print(fltd_tok_corp[0].similarity(fltd_tok_corp[1]))\n","print(fltd_tok_corp[0].similarity(fltd_tok_corp[2]))\n","print(fltd_tok_corp[1].similarity(fltd_tok_corp[2]))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["0.7297824993581993\n","0.5886927521791829\n","0.604620161888518\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ATk9SisbOBVy","colab_type":"text"},"source":["# Sentiment analysis"]},{"cell_type":"markdown","metadata":{"id":"QeCfPLpDOPFR","colab_type":"text"},"source":["## The dataset"]},{"cell_type":"markdown","metadata":{"id":"kGq2dvArzv5O","colab_type":"text"},"source":["### Run the code bellow :"]},{"cell_type":"markdown","metadata":{"id":"nO6c5rye0eH5","colab_type":"text"},"source":["We won't use the twitter dataset that you already know because as strong as embeddings are they aren't great with unknown words/abreviation/emoji and the twitter dataset is full of them.\n","\n","We will instead use a dataset with review from Amazon, Yelp and IMDB. "]},{"cell_type":"code","metadata":{"id":"cjs4j_2zOpWW","colab_type":"code","outputId":"3f256ab5-3039-43c4-a937-4d7020ec1e00","executionInfo":{"status":"ok","timestamp":1592203899826,"user_tz":-120,"elapsed":22225,"user":{"displayName":"Sébastien Vanstavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5NfGIDvXc-vdIHXhbJ3ezTYoxOfaUGnMRSECx5w=s64","userId":"10084807894626874113"}},"colab":{"base_uri":"https://localhost:8080/","height":406}},"source":["import pandas as pd\n","df_source = pd.read_csv(\"https://raw.githubusercontent.com/CindyAloui/datasets_wcs/master/sentiment_dataset.csv\", usecols=(\"sentence\", \"sentiment\", \"source\"))\n","df_source"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>sentiment</th>\n","      <th>source</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>So there is no way for me to plug it in here i...</td>\n","      <td>0</td>\n","      <td>amazon_cells_labelled</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Good case, Excellent value.</td>\n","      <td>1</td>\n","      <td>amazon_cells_labelled</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Great for the jawbone.</td>\n","      <td>1</td>\n","      <td>amazon_cells_labelled</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Tied to charger for conversations lasting more...</td>\n","      <td>0</td>\n","      <td>amazon_cells_labelled</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The mic is great.</td>\n","      <td>1</td>\n","      <td>amazon_cells_labelled</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2995</th>\n","      <td>I think food should have flavor and texture an...</td>\n","      <td>0</td>\n","      <td>yelp_labelled</td>\n","    </tr>\n","    <tr>\n","      <th>2996</th>\n","      <td>Appetite instantly gone.</td>\n","      <td>0</td>\n","      <td>yelp_labelled</td>\n","    </tr>\n","    <tr>\n","      <th>2997</th>\n","      <td>Overall I was not impressed and would not go b...</td>\n","      <td>0</td>\n","      <td>yelp_labelled</td>\n","    </tr>\n","    <tr>\n","      <th>2998</th>\n","      <td>The whole experience was underwhelming, and I ...</td>\n","      <td>0</td>\n","      <td>yelp_labelled</td>\n","    </tr>\n","    <tr>\n","      <th>2999</th>\n","      <td>Then, as if I hadn't wasted enough of my life ...</td>\n","      <td>0</td>\n","      <td>yelp_labelled</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3000 rows × 3 columns</p>\n","</div>"],"text/plain":["                                               sentence  ...                 source\n","0     So there is no way for me to plug it in here i...  ...  amazon_cells_labelled\n","1                           Good case, Excellent value.  ...  amazon_cells_labelled\n","2                                Great for the jawbone.  ...  amazon_cells_labelled\n","3     Tied to charger for conversations lasting more...  ...  amazon_cells_labelled\n","4                                     The mic is great.  ...  amazon_cells_labelled\n","...                                                 ...  ...                    ...\n","2995  I think food should have flavor and texture an...  ...          yelp_labelled\n","2996                           Appetite instantly gone.  ...          yelp_labelled\n","2997  Overall I was not impressed and would not go b...  ...          yelp_labelled\n","2998  The whole experience was underwhelming, and I ...  ...          yelp_labelled\n","2999  Then, as if I hadn't wasted enough of my life ...  ...          yelp_labelled\n","\n","[3000 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"62avNnxSyFSH","colab_type":"code","outputId":"4e2e2da3-44b5-41d8-86d8-4f100268a7de","executionInfo":{"status":"ok","timestamp":1592203899827,"user_tz":-120,"elapsed":22220,"user":{"displayName":"Sébastien Vanstavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5NfGIDvXc-vdIHXhbJ3ezTYoxOfaUGnMRSECx5w=s64","userId":"10084807894626874113"}},"colab":{"base_uri":"https://localhost:8080/","height":257}},"source":["df_source.groupby(['source', 'sentiment']).count()"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th>sentence</th>\n","    </tr>\n","    <tr>\n","      <th>source</th>\n","      <th>sentiment</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">amazon_cells_labelled</th>\n","      <th>0</th>\n","      <td>500</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>500</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">imdb_labelled</th>\n","      <th>0</th>\n","      <td>500</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>500</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">yelp_labelled</th>\n","      <th>0</th>\n","      <td>500</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>500</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                 sentence\n","source                sentiment          \n","amazon_cells_labelled 0               500\n","                      1               500\n","imdb_labelled         0               500\n","                      1               500\n","yelp_labelled         0               500\n","                      1               500"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"aZuQ4rBaOSXQ","colab_type":"text"},"source":["## Challenge"]},{"cell_type":"markdown","metadata":{"id":"VL_BaWDO0VAp","colab_type":"text"},"source":["Now you have all the elements to train a classifier for sentiment analysis using embeddings! A little reminder of the steps: \n","\n","- First take out the stop words so you won't have to do a weighted average. You can also lemmatize the text is you want but in this case it shouldn't have a big influence.\n","\n","- Then compute the sentence embeddings of the reviews. This is going to be our features.\n","\n","- Do a train test split.\n","\n","- Choose a type of classifier you want to use (for example a Logistic Regression).\n","\n","- Train and evaluate your classifier. \n","\n","You should be able to reach easily an accuracy of 80%."]},{"cell_type":"markdown","metadata":{"id":"M0MIToRxP0Bj","colab_type":"text"},"source":["### Words preprocessing"]},{"cell_type":"code","metadata":{"id":"7csk87eu3r0J","colab_type":"code","outputId":"1760c031-6803-416e-cf33-88df8818032c","executionInfo":{"status":"ok","timestamp":1592203900142,"user_tz":-120,"elapsed":22529,"user":{"displayName":"Sébastien Vanstavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5NfGIDvXc-vdIHXhbJ3ezTYoxOfaUGnMRSECx5w=s64","userId":"10084807894626874113"}},"colab":{"base_uri":"https://localhost:8080/","height":406}},"source":["df_source['filtred_sentence'] = [PunkStopWordsExterm(sent, 'sentence') for sent in df_source['sentence']]\n","df_source"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>sentiment</th>\n","      <th>source</th>\n","      <th>filtred_sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>So there is no way for me to plug it in here i...</td>\n","      <td>0</td>\n","      <td>amazon_cells_labelled</td>\n","      <td>So way plug US unless I go converter</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Good case, Excellent value.</td>\n","      <td>1</td>\n","      <td>amazon_cells_labelled</td>\n","      <td>Good case Excellent value</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Great for the jawbone.</td>\n","      <td>1</td>\n","      <td>amazon_cells_labelled</td>\n","      <td>Great jawbone</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Tied to charger for conversations lasting more...</td>\n","      <td>0</td>\n","      <td>amazon_cells_labelled</td>\n","      <td>Tied charger conversations lasting 45 minutes ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The mic is great.</td>\n","      <td>1</td>\n","      <td>amazon_cells_labelled</td>\n","      <td>The mic great</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2995</th>\n","      <td>I think food should have flavor and texture an...</td>\n","      <td>0</td>\n","      <td>yelp_labelled</td>\n","      <td>I think food flavor texture lacking</td>\n","    </tr>\n","    <tr>\n","      <th>2996</th>\n","      <td>Appetite instantly gone.</td>\n","      <td>0</td>\n","      <td>yelp_labelled</td>\n","      <td>Appetite instantly gone</td>\n","    </tr>\n","    <tr>\n","      <th>2997</th>\n","      <td>Overall I was not impressed and would not go b...</td>\n","      <td>0</td>\n","      <td>yelp_labelled</td>\n","      <td>Overall I impressed would go back</td>\n","    </tr>\n","    <tr>\n","      <th>2998</th>\n","      <td>The whole experience was underwhelming, and I ...</td>\n","      <td>0</td>\n","      <td>yelp_labelled</td>\n","      <td>The whole experience underwhelming I think go ...</td>\n","    </tr>\n","    <tr>\n","      <th>2999</th>\n","      <td>Then, as if I hadn't wasted enough of my life ...</td>\n","      <td>0</td>\n","      <td>yelp_labelled</td>\n","      <td>Then I wasted enough life poured salt wound dr...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3000 rows × 4 columns</p>\n","</div>"],"text/plain":["                                               sentence  ...                                   filtred_sentence\n","0     So there is no way for me to plug it in here i...  ...               So way plug US unless I go converter\n","1                           Good case, Excellent value.  ...                          Good case Excellent value\n","2                                Great for the jawbone.  ...                                      Great jawbone\n","3     Tied to charger for conversations lasting more...  ...  Tied charger conversations lasting 45 minutes ...\n","4                                     The mic is great.  ...                                      The mic great\n","...                                                 ...  ...                                                ...\n","2995  I think food should have flavor and texture an...  ...                I think food flavor texture lacking\n","2996                           Appetite instantly gone.  ...                            Appetite instantly gone\n","2997  Overall I was not impressed and would not go b...  ...                  Overall I impressed would go back\n","2998  The whole experience was underwhelming, and I ...  ...  The whole experience underwhelming I think go ...\n","2999  Then, as if I hadn't wasted enough of my life ...  ...  Then I wasted enough life poured salt wound dr...\n","\n","[3000 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"V6K7lB-NzeIq","colab_type":"text"},"source":["### Embedding sentences\n"]},{"cell_type":"code","metadata":{"id":"vQi32RW0xhNR","colab_type":"code","outputId":"1f97352c-313d-4154-8d52-ccc09fc7a1ea","executionInfo":{"status":"ok","timestamp":1592203922689,"user_tz":-120,"elapsed":45071,"user":{"displayName":"Sébastien Vanstavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5NfGIDvXc-vdIHXhbJ3ezTYoxOfaUGnMRSECx5w=s64","userId":"10084807894626874113"}},"colab":{"base_uri":"https://localhost:8080/","height":406}},"source":["df_source['embedding_sentence'] = [nlp(sent).vector for sent in df_source['filtred_sentence']]\n","df_source"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>sentiment</th>\n","      <th>source</th>\n","      <th>filtred_sentence</th>\n","      <th>embedding_sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>So there is no way for me to plug it in here i...</td>\n","      <td>0</td>\n","      <td>amazon_cells_labelled</td>\n","      <td>So way plug US unless I go converter</td>\n","      <td>[0.1096631, 0.16896625, -0.14765373, 0.0498342...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Good case, Excellent value.</td>\n","      <td>1</td>\n","      <td>amazon_cells_labelled</td>\n","      <td>Good case Excellent value</td>\n","      <td>[-0.27085474, 0.560745, -0.16895999, -0.057802...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Great for the jawbone.</td>\n","      <td>1</td>\n","      <td>amazon_cells_labelled</td>\n","      <td>Great jawbone</td>\n","      <td>[0.013746999, 0.37386, -0.0674105, -0.074564, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Tied to charger for conversations lasting more...</td>\n","      <td>0</td>\n","      <td>amazon_cells_labelled</td>\n","      <td>Tied charger conversations lasting 45 minutes ...</td>\n","      <td>[0.0052317046, 0.263143, 0.038863745, 0.032554...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The mic is great.</td>\n","      <td>1</td>\n","      <td>amazon_cells_labelled</td>\n","      <td>The mic great</td>\n","      <td>[-0.015982, 0.35231665, 0.025606329, 0.1886543...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2995</th>\n","      <td>I think food should have flavor and texture an...</td>\n","      <td>0</td>\n","      <td>yelp_labelled</td>\n","      <td>I think food flavor texture lacking</td>\n","      <td>[-0.13820933, 0.23190516, -0.01509251, -0.2692...</td>\n","    </tr>\n","    <tr>\n","      <th>2996</th>\n","      <td>Appetite instantly gone.</td>\n","      <td>0</td>\n","      <td>yelp_labelled</td>\n","      <td>Appetite instantly gone</td>\n","      <td>[-0.10598, 0.18677, -0.03493534, -0.06977666, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2997</th>\n","      <td>Overall I was not impressed and would not go b...</td>\n","      <td>0</td>\n","      <td>yelp_labelled</td>\n","      <td>Overall I impressed would go back</td>\n","      <td>[0.03134817, 0.32979402, -0.22557668, -0.00845...</td>\n","    </tr>\n","    <tr>\n","      <th>2998</th>\n","      <td>The whole experience was underwhelming, and I ...</td>\n","      <td>0</td>\n","      <td>yelp_labelled</td>\n","      <td>The whole experience underwhelming I think go ...</td>\n","      <td>[0.062392544, 0.003429463, -0.03594419, -0.126...</td>\n","    </tr>\n","    <tr>\n","      <th>2999</th>\n","      <td>Then, as if I hadn't wasted enough of my life ...</td>\n","      <td>0</td>\n","      <td>yelp_labelled</td>\n","      <td>Then I wasted enough life poured salt wound dr...</td>\n","      <td>[0.03812315, 0.15376453, -0.025169887, -0.0376...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3000 rows × 5 columns</p>\n","</div>"],"text/plain":["                                               sentence  ...                                 embedding_sentence\n","0     So there is no way for me to plug it in here i...  ...  [0.1096631, 0.16896625, -0.14765373, 0.0498342...\n","1                           Good case, Excellent value.  ...  [-0.27085474, 0.560745, -0.16895999, -0.057802...\n","2                                Great for the jawbone.  ...  [0.013746999, 0.37386, -0.0674105, -0.074564, ...\n","3     Tied to charger for conversations lasting more...  ...  [0.0052317046, 0.263143, 0.038863745, 0.032554...\n","4                                     The mic is great.  ...  [-0.015982, 0.35231665, 0.025606329, 0.1886543...\n","...                                                 ...  ...                                                ...\n","2995  I think food should have flavor and texture an...  ...  [-0.13820933, 0.23190516, -0.01509251, -0.2692...\n","2996                           Appetite instantly gone.  ...  [-0.10598, 0.18677, -0.03493534, -0.06977666, ...\n","2997  Overall I was not impressed and would not go b...  ...  [0.03134817, 0.32979402, -0.22557668, -0.00845...\n","2998  The whole experience was underwhelming, and I ...  ...  [0.062392544, 0.003429463, -0.03594419, -0.126...\n","2999  Then, as if I hadn't wasted enough of my life ...  ...  [0.03812315, 0.15376453, -0.025169887, -0.0376...\n","\n","[3000 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"UJn84u4XQHdH","colab_type":"text"},"source":["### Modelisation"]},{"cell_type":"code","metadata":{"id":"1-BXqU3JpKj3","colab_type":"code","colab":{}},"source":["def EmbeddingMatrix(data):\n","  '''\n","  Cette fonction permet de transformer une liste matrice de liste DataFrame avec \n","  autant de colonne que d'éléments dans la liste et autant de lignes que de \n","  listes.\n","  e.g., \n","      [1, 2, 3, 4, 5],\n","      [6, 7, 8, 9, 10]    (2, 1)\n","\n","       | 0| 1| 2| 3| 4|\n","      0| 1| 2| 3| 4| 5|\n","      1| 6| 7| 8| 9|10|   (2, 5)\n","  '''\n","  \n","  import numpy as np\n","\n","  df = pd.DataFrame(np.zeros((3000,300)))\n","\n","  for col in range(len(data.iloc[0, 0])):\n","    for row in range(data.shape[0]):\n","      df.iloc[row, col] = data.iloc[row, 0][col]\n","\n","  return df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qKQCKH01LB_f","colab_type":"code","outputId":"7d0b4bad-fbc6-4a84-fce4-0667497a59e2","executionInfo":{"status":"ok","timestamp":1592204079730,"user_tz":-120,"elapsed":129550,"user":{"displayName":"Sébastien Vanstavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5NfGIDvXc-vdIHXhbJ3ezTYoxOfaUGnMRSECx5w=s64","userId":"10084807894626874113"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X = EmbeddingMatrix(df_source[['embedding_sentence']])\n","X.shape"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3000, 300)"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"YqfkFfaqsH_2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":246},"outputId":"18f65b6b-6db6-437a-8fb1-67d0b9ce7ad8","executionInfo":{"status":"ok","timestamp":1592204079731,"user_tz":-120,"elapsed":124380,"user":{"displayName":"Sébastien Vanstavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5NfGIDvXc-vdIHXhbJ3ezTYoxOfaUGnMRSECx5w=s64","userId":"10084807894626874113"}}},"source":["X.head()"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","      <th>26</th>\n","      <th>27</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","      <th>31</th>\n","      <th>32</th>\n","      <th>33</th>\n","      <th>34</th>\n","      <th>35</th>\n","      <th>36</th>\n","      <th>37</th>\n","      <th>38</th>\n","      <th>39</th>\n","      <th>...</th>\n","      <th>260</th>\n","      <th>261</th>\n","      <th>262</th>\n","      <th>263</th>\n","      <th>264</th>\n","      <th>265</th>\n","      <th>266</th>\n","      <th>267</th>\n","      <th>268</th>\n","      <th>269</th>\n","      <th>270</th>\n","      <th>271</th>\n","      <th>272</th>\n","      <th>273</th>\n","      <th>274</th>\n","      <th>275</th>\n","      <th>276</th>\n","      <th>277</th>\n","      <th>278</th>\n","      <th>279</th>\n","      <th>280</th>\n","      <th>281</th>\n","      <th>282</th>\n","      <th>283</th>\n","      <th>284</th>\n","      <th>285</th>\n","      <th>286</th>\n","      <th>287</th>\n","      <th>288</th>\n","      <th>289</th>\n","      <th>290</th>\n","      <th>291</th>\n","      <th>292</th>\n","      <th>293</th>\n","      <th>294</th>\n","      <th>295</th>\n","      <th>296</th>\n","      <th>297</th>\n","      <th>298</th>\n","      <th>299</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.109663</td>\n","      <td>0.168966</td>\n","      <td>-0.147654</td>\n","      <td>0.049834</td>\n","      <td>0.102829</td>\n","      <td>-0.094142</td>\n","      <td>0.109589</td>\n","      <td>-0.206444</td>\n","      <td>-0.027967</td>\n","      <td>1.784350</td>\n","      <td>-0.124273</td>\n","      <td>0.063895</td>\n","      <td>0.116049</td>\n","      <td>-0.084630</td>\n","      <td>-0.187175</td>\n","      <td>-0.153012</td>\n","      <td>-0.262444</td>\n","      <td>1.480843</td>\n","      <td>-0.136094</td>\n","      <td>0.134874</td>\n","      <td>0.091645</td>\n","      <td>-0.016464</td>\n","      <td>-0.099847</td>\n","      <td>0.014625</td>\n","      <td>-0.079578</td>\n","      <td>0.051355</td>\n","      <td>-0.166281</td>\n","      <td>-0.316079</td>\n","      <td>0.169389</td>\n","      <td>-0.164995</td>\n","      <td>-0.185009</td>\n","      <td>0.054753</td>\n","      <td>0.040482</td>\n","      <td>0.249062</td>\n","      <td>-0.014022</td>\n","      <td>-0.045909</td>\n","      <td>0.140762</td>\n","      <td>0.061508</td>\n","      <td>-0.145026</td>\n","      <td>-0.297461</td>\n","      <td>...</td>\n","      <td>0.027680</td>\n","      <td>0.090858</td>\n","      <td>0.105971</td>\n","      <td>-0.173263</td>\n","      <td>0.236921</td>\n","      <td>-0.167250</td>\n","      <td>-0.002762</td>\n","      <td>0.043350</td>\n","      <td>0.450269</td>\n","      <td>0.249652</td>\n","      <td>0.217144</td>\n","      <td>0.051456</td>\n","      <td>-0.009744</td>\n","      <td>0.111146</td>\n","      <td>-0.022863</td>\n","      <td>0.169901</td>\n","      <td>-0.008170</td>\n","      <td>-0.035847</td>\n","      <td>-0.151876</td>\n","      <td>0.112259</td>\n","      <td>0.026790</td>\n","      <td>-0.004259</td>\n","      <td>0.028025</td>\n","      <td>0.170896</td>\n","      <td>-0.062507</td>\n","      <td>-0.056613</td>\n","      <td>0.102537</td>\n","      <td>-0.131265</td>\n","      <td>0.158625</td>\n","      <td>-0.062705</td>\n","      <td>-0.187955</td>\n","      <td>-0.053995</td>\n","      <td>-0.156746</td>\n","      <td>-0.033300</td>\n","      <td>0.137580</td>\n","      <td>-0.061015</td>\n","      <td>-0.023025</td>\n","      <td>-0.073138</td>\n","      <td>-0.014318</td>\n","      <td>0.085721</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.270855</td>\n","      <td>0.560745</td>\n","      <td>-0.168960</td>\n","      <td>-0.057803</td>\n","      <td>0.036918</td>\n","      <td>0.014717</td>\n","      <td>0.108985</td>\n","      <td>-0.328349</td>\n","      <td>-0.006825</td>\n","      <td>1.993950</td>\n","      <td>-0.230334</td>\n","      <td>0.130527</td>\n","      <td>0.210568</td>\n","      <td>0.095102</td>\n","      <td>-0.238488</td>\n","      <td>-0.164150</td>\n","      <td>-0.185345</td>\n","      <td>1.642375</td>\n","      <td>-0.046432</td>\n","      <td>-0.165923</td>\n","      <td>-0.325705</td>\n","      <td>-0.017494</td>\n","      <td>0.082217</td>\n","      <td>-0.191642</td>\n","      <td>0.168568</td>\n","      <td>0.072748</td>\n","      <td>0.059030</td>\n","      <td>-0.097233</td>\n","      <td>-0.045005</td>\n","      <td>-0.187548</td>\n","      <td>0.196536</td>\n","      <td>-0.076026</td>\n","      <td>0.186652</td>\n","      <td>0.090627</td>\n","      <td>0.001399</td>\n","      <td>-0.008536</td>\n","      <td>0.276623</td>\n","      <td>-0.215912</td>\n","      <td>-0.160085</td>\n","      <td>-0.381673</td>\n","      <td>...</td>\n","      <td>0.086577</td>\n","      <td>0.115545</td>\n","      <td>0.132033</td>\n","      <td>0.322740</td>\n","      <td>0.206147</td>\n","      <td>-0.033628</td>\n","      <td>0.181212</td>\n","      <td>0.232860</td>\n","      <td>0.536573</td>\n","      <td>-0.112325</td>\n","      <td>-0.040972</td>\n","      <td>-0.049456</td>\n","      <td>-0.040866</td>\n","      <td>-0.282870</td>\n","      <td>-0.062280</td>\n","      <td>0.153102</td>\n","      <td>0.062588</td>\n","      <td>-0.054287</td>\n","      <td>-0.089045</td>\n","      <td>0.262835</td>\n","      <td>-0.239125</td>\n","      <td>0.083166</td>\n","      <td>0.003665</td>\n","      <td>-0.139467</td>\n","      <td>-0.026252</td>\n","      <td>-0.048946</td>\n","      <td>-0.018232</td>\n","      <td>-0.028860</td>\n","      <td>0.032971</td>\n","      <td>0.134638</td>\n","      <td>-0.312330</td>\n","      <td>0.200558</td>\n","      <td>-0.281409</td>\n","      <td>-0.127516</td>\n","      <td>0.184591</td>\n","      <td>-0.139078</td>\n","      <td>0.031646</td>\n","      <td>-0.389430</td>\n","      <td>-0.034399</td>\n","      <td>0.036702</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.013747</td>\n","      <td>0.373860</td>\n","      <td>-0.067410</td>\n","      <td>-0.074564</td>\n","      <td>0.070689</td>\n","      <td>-0.153540</td>\n","      <td>0.143205</td>\n","      <td>-0.139250</td>\n","      <td>-0.083925</td>\n","      <td>1.703450</td>\n","      <td>0.011520</td>\n","      <td>-0.040388</td>\n","      <td>-0.070225</td>\n","      <td>0.371185</td>\n","      <td>0.084710</td>\n","      <td>0.052880</td>\n","      <td>-0.330130</td>\n","      <td>1.555150</td>\n","      <td>-0.064726</td>\n","      <td>0.236630</td>\n","      <td>-0.032779</td>\n","      <td>-0.153369</td>\n","      <td>0.024040</td>\n","      <td>-0.350315</td>\n","      <td>0.071991</td>\n","      <td>-0.095150</td>\n","      <td>0.177640</td>\n","      <td>-0.037675</td>\n","      <td>-0.155860</td>\n","      <td>0.054762</td>\n","      <td>0.145335</td>\n","      <td>-0.132815</td>\n","      <td>0.101590</td>\n","      <td>-0.222905</td>\n","      <td>0.378795</td>\n","      <td>0.051050</td>\n","      <td>0.152127</td>\n","      <td>-0.215993</td>\n","      <td>0.029535</td>\n","      <td>0.079135</td>\n","      <td>...</td>\n","      <td>-0.008265</td>\n","      <td>-0.073052</td>\n","      <td>0.013750</td>\n","      <td>0.088435</td>\n","      <td>0.397781</td>\n","      <td>0.240161</td>\n","      <td>0.069705</td>\n","      <td>0.028425</td>\n","      <td>0.037620</td>\n","      <td>0.214086</td>\n","      <td>0.155990</td>\n","      <td>0.323997</td>\n","      <td>-0.487735</td>\n","      <td>0.060985</td>\n","      <td>0.008515</td>\n","      <td>-0.029136</td>\n","      <td>-0.327200</td>\n","      <td>0.092521</td>\n","      <td>-0.219740</td>\n","      <td>0.171205</td>\n","      <td>-0.111631</td>\n","      <td>0.208718</td>\n","      <td>-0.125098</td>\n","      <td>-0.213375</td>\n","      <td>0.078640</td>\n","      <td>-0.136414</td>\n","      <td>0.201600</td>\n","      <td>-0.088900</td>\n","      <td>-0.024840</td>\n","      <td>-0.144730</td>\n","      <td>-0.507210</td>\n","      <td>0.132917</td>\n","      <td>-0.302480</td>\n","      <td>0.071517</td>\n","      <td>0.254980</td>\n","      <td>0.315783</td>\n","      <td>0.293955</td>\n","      <td>0.137308</td>\n","      <td>0.143229</td>\n","      <td>0.105892</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.005232</td>\n","      <td>0.263143</td>\n","      <td>0.038864</td>\n","      <td>0.032554</td>\n","      <td>0.052446</td>\n","      <td>-0.300596</td>\n","      <td>-0.138098</td>\n","      <td>0.033495</td>\n","      <td>0.276415</td>\n","      <td>1.736114</td>\n","      <td>0.069309</td>\n","      <td>0.104837</td>\n","      <td>0.194245</td>\n","      <td>-0.129325</td>\n","      <td>-0.135233</td>\n","      <td>-0.014320</td>\n","      <td>-0.044309</td>\n","      <td>1.289629</td>\n","      <td>-0.196687</td>\n","      <td>0.200776</td>\n","      <td>-0.142310</td>\n","      <td>0.194908</td>\n","      <td>-0.068316</td>\n","      <td>-0.107279</td>\n","      <td>0.242936</td>\n","      <td>0.004401</td>\n","      <td>0.103688</td>\n","      <td>0.015258</td>\n","      <td>-0.016261</td>\n","      <td>0.116775</td>\n","      <td>0.053180</td>\n","      <td>0.145179</td>\n","      <td>-0.297973</td>\n","      <td>-0.241276</td>\n","      <td>0.032697</td>\n","      <td>0.187086</td>\n","      <td>0.081994</td>\n","      <td>-0.084893</td>\n","      <td>0.067034</td>\n","      <td>0.016117</td>\n","      <td>...</td>\n","      <td>0.140689</td>\n","      <td>0.044818</td>\n","      <td>0.082057</td>\n","      <td>0.025478</td>\n","      <td>-0.018318</td>\n","      <td>-0.155836</td>\n","      <td>0.011366</td>\n","      <td>0.168235</td>\n","      <td>0.142625</td>\n","      <td>-0.144056</td>\n","      <td>0.074373</td>\n","      <td>-0.003572</td>\n","      <td>0.038383</td>\n","      <td>-0.064905</td>\n","      <td>-0.139747</td>\n","      <td>0.105948</td>\n","      <td>0.097436</td>\n","      <td>0.102608</td>\n","      <td>0.020270</td>\n","      <td>0.063612</td>\n","      <td>-0.114077</td>\n","      <td>0.020457</td>\n","      <td>0.066293</td>\n","      <td>0.088737</td>\n","      <td>-0.007222</td>\n","      <td>0.119746</td>\n","      <td>0.079966</td>\n","      <td>-0.172343</td>\n","      <td>0.044939</td>\n","      <td>0.192429</td>\n","      <td>-0.198691</td>\n","      <td>-0.003271</td>\n","      <td>-0.066565</td>\n","      <td>0.127852</td>\n","      <td>0.111966</td>\n","      <td>0.010233</td>\n","      <td>0.007898</td>\n","      <td>-0.068726</td>\n","      <td>-0.098732</td>\n","      <td>0.016123</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.015982</td>\n","      <td>0.352317</td>\n","      <td>0.025606</td>\n","      <td>0.188654</td>\n","      <td>0.182467</td>\n","      <td>0.060571</td>\n","      <td>0.233850</td>\n","      <td>-0.234687</td>\n","      <td>0.133407</td>\n","      <td>1.682603</td>\n","      <td>-0.091687</td>\n","      <td>-0.039279</td>\n","      <td>0.321583</td>\n","      <td>-0.011060</td>\n","      <td>-0.005474</td>\n","      <td>-0.034167</td>\n","      <td>-0.262670</td>\n","      <td>1.128467</td>\n","      <td>-0.277277</td>\n","      <td>0.081237</td>\n","      <td>-0.068542</td>\n","      <td>-0.027280</td>\n","      <td>0.082719</td>\n","      <td>-0.169420</td>\n","      <td>0.046568</td>\n","      <td>-0.104087</td>\n","      <td>-0.250583</td>\n","      <td>0.249136</td>\n","      <td>-0.066547</td>\n","      <td>0.076403</td>\n","      <td>-0.100633</td>\n","      <td>0.309870</td>\n","      <td>0.060000</td>\n","      <td>0.265996</td>\n","      <td>-0.031950</td>\n","      <td>0.006489</td>\n","      <td>0.114733</td>\n","      <td>-0.127352</td>\n","      <td>-0.208155</td>\n","      <td>-0.248147</td>\n","      <td>...</td>\n","      <td>-0.097483</td>\n","      <td>-0.060752</td>\n","      <td>0.156287</td>\n","      <td>-0.131477</td>\n","      <td>-0.005229</td>\n","      <td>-0.036176</td>\n","      <td>0.261663</td>\n","      <td>0.155129</td>\n","      <td>0.282853</td>\n","      <td>0.050193</td>\n","      <td>0.129882</td>\n","      <td>0.120452</td>\n","      <td>-0.366380</td>\n","      <td>-0.101330</td>\n","      <td>0.086427</td>\n","      <td>-0.087886</td>\n","      <td>0.047033</td>\n","      <td>0.131491</td>\n","      <td>-0.069938</td>\n","      <td>-0.126760</td>\n","      <td>0.078902</td>\n","      <td>0.126602</td>\n","      <td>-0.106804</td>\n","      <td>0.116491</td>\n","      <td>0.047214</td>\n","      <td>-0.068193</td>\n","      <td>0.333480</td>\n","      <td>0.012526</td>\n","      <td>0.005040</td>\n","      <td>-0.006707</td>\n","      <td>-0.495020</td>\n","      <td>0.177245</td>\n","      <td>-0.094147</td>\n","      <td>-0.074577</td>\n","      <td>0.040951</td>\n","      <td>0.139583</td>\n","      <td>0.196097</td>\n","      <td>0.010625</td>\n","      <td>-0.051216</td>\n","      <td>0.259620</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 300 columns</p>\n","</div>"],"text/plain":["        0         1         2    ...       297       298       299\n","0  0.109663  0.168966 -0.147654  ... -0.073138 -0.014318  0.085721\n","1 -0.270855  0.560745 -0.168960  ... -0.389430 -0.034399  0.036702\n","2  0.013747  0.373860 -0.067410  ...  0.137308  0.143229  0.105892\n","3  0.005232  0.263143  0.038864  ... -0.068726 -0.098732  0.016123\n","4 -0.015982  0.352317  0.025606  ...  0.010625 -0.051216  0.259620\n","\n","[5 rows x 300 columns]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"y4ckboALPTB2","colab_type":"code","outputId":"5908675a-80ed-4710-93d9-e613b6f74b0a","executionInfo":{"status":"ok","timestamp":1592204079731,"user_tz":-120,"elapsed":123882,"user":{"displayName":"Sébastien Vanstavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5NfGIDvXc-vdIHXhbJ3ezTYoxOfaUGnMRSECx5w=s64","userId":"10084807894626874113"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["y = df_source['sentiment']\n","y.shape"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3000,)"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"Xgd8llArQOH-","colab_type":"text"},"source":["#### Model training\n"]},{"cell_type":"code","metadata":{"id":"zsrOdscUQQ1C","colab_type":"code","colab":{}},"source":["def LogRegModeling(X, y, rand_state=42, train_size=.8):\n","\n","  from sklearn.model_selection import train_test_split\n","  from sklearn.preprocessing import StandardScaler\n","  from sklearn.linear_model import LogisticRegression\n","    \n","  X_train, X_test, y_train, y_test = train_test_split(X, y, \n","                                                      random_state = rand_state, \n","                                                      train_size = train_size)\n","  LogisticModel = LogisticRegression().fit(X_train, y_train)\n","\n","  print(\"Model accuracy score on train set =\", \n","        LogisticModel.score(X_train, y_train))\n","  print(\"Model accuracy score on test set =\", \n","        LogisticModel.score(X_test, y_test))\n","  \n","  return LogisticModel\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R6GlOV2ESJ_l","colab_type":"code","outputId":"ea4ede9c-384b-441d-df2b-3f491f37968c","executionInfo":{"status":"ok","timestamp":1592204257494,"user_tz":-120,"elapsed":679,"user":{"displayName":"Sébastien Vanstavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5NfGIDvXc-vdIHXhbJ3ezTYoxOfaUGnMRSECx5w=s64","userId":"10084807894626874113"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["LogRModel = LogRegModeling(X, y, rand_state = 42)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Model accuracy score on train set = 0.8745833333333334\n","Model accuracy score on test set = 0.8233333333333334\n"],"name":"stdout"}]}]}